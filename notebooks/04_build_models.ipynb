{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944705f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üì¶ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da48793",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "courses = pd.read_csv('../data/processed/courses_with_id.csv')\n",
    "users = pd.read_csv('../data/processed/synthetic_users.csv')\n",
    "interactions = pd.read_csv('../data/processed/user_interactions.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded data:\")\n",
    "print(f\"   Courses: {len(courses):,}\")\n",
    "print(f\"   Users: {len(users):,}\")\n",
    "print(f\"   Interactions: {len(interactions):,}\")\n",
    "print(f\"   Sparsity: {(1 - len(interactions) / (len(users) * len(courses))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfec40",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "Split interactions into training (80%) and testing (20%) sets.\n",
    "We use stratified split to ensure each user has data in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÄ Splitting data into train/test sets...\\n\")\n",
    "\n",
    "# Group by user to ensure each user has interactions in both train and test\n",
    "train_interactions = []\n",
    "test_interactions = []\n",
    "\n",
    "for user_id in interactions['user_id'].unique():\n",
    "    user_data = interactions[interactions['user_id'] == user_id]\n",
    "    \n",
    "    # Split 80/20 for this user\n",
    "    if len(user_data) >= 5:  # Only split if user has enough interactions\n",
    "        train, test = train_test_split(user_data, test_size=0.2, random_state=42)\n",
    "        train_interactions.append(train)\n",
    "        test_interactions.append(test)\n",
    "    else:\n",
    "        # For users with <5 interactions, put all in train\n",
    "        train_interactions.append(user_data)\n",
    "\n",
    "train_df = pd.concat(train_interactions, ignore_index=True)\n",
    "test_df = pd.concat(test_interactions, ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Split complete:\")\n",
    "print(f\"   Training interactions: {len(train_df):,} ({len(train_df)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"   Testing interactions: {len(test_df):,} ({len(test_df)/len(interactions)*100:.1f}%)\")\n",
    "print(f\"   Users in train: {train_df['user_id'].nunique()}\")\n",
    "print(f\"   Users in test: {test_df['user_id'].nunique()}\")\n",
    "print(f\"   Courses in train: {train_df['course_id'].nunique()}\")\n",
    "print(f\"   Courses in test: {test_df['course_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2acb66",
   "metadata": {},
   "source": [
    "## 3. Content-Based Filtering Model\n",
    "\n",
    "Uses TF-IDF to vectorize course skills and computes similarity between courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Building Content-Based Filtering Model...\\n\")\n",
    "\n",
    "# Create TF-IDF vectors from course skills\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,  # Top 500 skills\n",
    "    min_df=2,          # Skill must appear in at least 2 courses\n",
    "    ngram_range=(1, 2) # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit on skills_cleaned column\n",
    "skill_vectors = vectorizer.fit_transform(courses['skills_cleaned'].fillna(''))\n",
    "\n",
    "print(f\"   TF-IDF matrix shape: {skill_vectors.shape}\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Compute course-course similarity matrix\n",
    "print(f\"\\n   Computing course similarity matrix...\")\n",
    "course_similarity = cosine_similarity(skill_vectors)\n",
    "\n",
    "print(f\"   ‚úÖ Similarity matrix: {course_similarity.shape}\")\n",
    "print(f\"   Avg similarity: {course_similarity.mean():.4f}\")\n",
    "print(f\"   Max similarity (excluding self): {np.sort(course_similarity.flatten())[-len(courses)-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac239ca7",
   "metadata": {},
   "source": [
    "### 3.1 Content-Based Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recommend(user_id, train_data, courses_df, similarity_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommend courses based on content similarity to courses user has interacted with.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID\n",
    "        train_data: Training interaction data\n",
    "        courses_df: Course dataframe\n",
    "        similarity_matrix: Course-course similarity matrix\n",
    "        top_n: Number of recommendations\n",
    "    \n",
    "    Returns:\n",
    "        List of (course_id, score) tuples\n",
    "    \"\"\"\n",
    "    # Get courses user has interacted with\n",
    "    user_courses = train_data[train_data['user_id'] == user_id]['course_id'].values\n",
    "    \n",
    "    if len(user_courses) == 0:\n",
    "        return []  # Cold start - no history\n",
    "    \n",
    "    # Compute scores for all courses\n",
    "    course_scores = np.zeros(len(courses_df))\n",
    "    \n",
    "    for course_id in user_courses:\n",
    "        # Add similarity scores from this course\n",
    "        course_scores += similarity_matrix[course_id]\n",
    "    \n",
    "    # Average the scores\n",
    "    course_scores = course_scores / len(user_courses)\n",
    "    \n",
    "    # Remove courses user already interacted with\n",
    "    course_scores[user_courses] = -1\n",
    "    \n",
    "    # Get top N\n",
    "    top_indices = np.argsort(course_scores)[::-1][:top_n]\n",
    "    recommendations = [(idx, course_scores[idx]) for idx in top_indices if course_scores[idx] > 0]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test with a sample user\n",
    "sample_user = 0\n",
    "sample_recs = content_based_recommend(sample_user, train_df, courses, course_similarity, top_n=5)\n",
    "\n",
    "print(f\"\\nüìù Sample recommendations for User {sample_user}:\")\n",
    "for course_id, score in sample_recs:\n",
    "    course_name = courses[courses['course_id'] == course_id]['Course Name'].values[0]\n",
    "    print(f\"   {course_name[:50]}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd49ff7",
   "metadata": {},
   "source": [
    "## 4. Collaborative Filtering Model\n",
    "\n",
    "Uses matrix factorization (SVD) to learn latent features from user-course interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ù Building Collaborative Filtering Model...\\n\")\n",
    "\n",
    "try:\n",
    "    from surprise import SVD, Dataset, Reader\n",
    "    from surprise.model_selection import cross_validate\n",
    "    print(\"   ‚úÖ Surprise library loaded\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  Installing surprise library...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"scikit-surprise\"])\n",
    "    from surprise import SVD, Dataset, Reader\n",
    "    from surprise.model_selection import cross_validate\n",
    "    print(\"   ‚úÖ Surprise library installed and loaded\")\n",
    "\n",
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(0, 1))  # Binary interactions\n",
    "\n",
    "# Create dataset from train_df\n",
    "train_surprise = Dataset.load_from_df(\n",
    "    train_df[['user_id', 'course_id', 'interaction']], \n",
    "    reader\n",
    ")\n",
    "\n",
    "# Build full trainset\n",
    "trainset = train_surprise.build_full_trainset()\n",
    "\n",
    "print(f\"   Training set: {trainset.n_users} users, {trainset.n_items} courses\")\n",
    "print(f\"   Total ratings: {trainset.n_ratings}\")\n",
    "\n",
    "# Train SVD model\n",
    "print(f\"\\n   Training SVD model...\")\n",
    "svd_model = SVD(\n",
    "    n_factors=50,      # Latent features\n",
    "    n_epochs=20,       # Training iterations\n",
    "    lr_all=0.005,      # Learning rate\n",
    "    reg_all=0.02,      # Regularization\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "print(f\"   ‚úÖ SVD model trained!\")\n",
    "print(f\"   Latent factors: {svd_model.n_factors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5dd525",
   "metadata": {},
   "source": [
    "### 4.1 Collaborative Filtering Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53321114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_recommend(user_id, train_data, courses_df, model, top_n=10):\n",
    "    \"\"\"\n",
    "    Recommend courses using collaborative filtering.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID\n",
    "        train_data: Training interaction data\n",
    "        courses_df: Course dataframe\n",
    "        model: Trained SVD model\n",
    "        top_n: Number of recommendations\n",
    "    \n",
    "    Returns:\n",
    "        List of (course_id, score) tuples\n",
    "    \"\"\"\n",
    "    # Get courses user has already interacted with\n",
    "    user_courses = set(train_data[train_data['user_id'] == user_id]['course_id'].values)\n",
    "    \n",
    "    # Predict for all courses\n",
    "    all_courses = courses_df['course_id'].values\n",
    "    predictions = []\n",
    "    \n",
    "    for course_id in all_courses:\n",
    "        if course_id not in user_courses:\n",
    "            pred = model.predict(user_id, course_id)\n",
    "            predictions.append((course_id, pred.est))\n",
    "    \n",
    "    # Sort by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return predictions[:top_n]\n",
    "\n",
    "# Test with sample user\n",
    "sample_recs_cf = collaborative_recommend(sample_user, train_df, courses, svd_model, top_n=5)\n",
    "\n",
    "print(f\"\\nüìù CF recommendations for User {sample_user}:\")\n",
    "for course_id, score in sample_recs_cf:\n",
    "    course_name = courses[courses['course_id'] == course_id]['Course Name'].values[0]\n",
    "    print(f\"   {course_name[:50]}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cc43cd",
   "metadata": {},
   "source": [
    "## 5. Hybrid Recommender Model\n",
    "\n",
    "Combines content-based and collaborative filtering predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e420dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Building Hybrid Recommender Model...\\n\")\n",
    "\n",
    "def hybrid_recommend(user_id, train_data, courses_df, similarity_matrix, cf_model, \n",
    "                     content_weight=0.6, cf_weight=0.4, top_n=10):\n",
    "    \"\"\"\n",
    "    Hybrid recommender combining content-based and collaborative filtering.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID\n",
    "        train_data: Training interaction data\n",
    "        courses_df: Course dataframe\n",
    "        similarity_matrix: Course-course similarity matrix\n",
    "        cf_model: Trained collaborative filtering model\n",
    "        content_weight: Weight for content-based score (0-1)\n",
    "        cf_weight: Weight for CF score (0-1)\n",
    "        top_n: Number of recommendations\n",
    "    \n",
    "    Returns:\n",
    "        List of (course_id, score, content_score, cf_score) tuples\n",
    "    \"\"\"\n",
    "    # Get recommendations from both models\n",
    "    content_recs = content_based_recommend(user_id, train_data, courses_df, \n",
    "                                          similarity_matrix, top_n=50)\n",
    "    cf_recs = collaborative_recommend(user_id, train_data, courses_df, \n",
    "                                     cf_model, top_n=50)\n",
    "    \n",
    "    # Normalize scores to 0-1 range\n",
    "    if content_recs:\n",
    "        max_content = max(score for _, score in content_recs)\n",
    "        content_dict = {cid: score/max_content for cid, score in content_recs}\n",
    "    else:\n",
    "        content_dict = {}\n",
    "    \n",
    "    if cf_recs:\n",
    "        max_cf = max(score for _, score in cf_recs)\n",
    "        min_cf = min(score for _, score in cf_recs)\n",
    "        cf_dict = {cid: (score - min_cf)/(max_cf - min_cf) if max_cf > min_cf else 0 \n",
    "                   for cid, score in cf_recs}\n",
    "    else:\n",
    "        cf_dict = {}\n",
    "    \n",
    "    # Combine scores\n",
    "    all_courses = set(content_dict.keys()) | set(cf_dict.keys())\n",
    "    hybrid_scores = []\n",
    "    \n",
    "    for course_id in all_courses:\n",
    "        content_score = content_dict.get(course_id, 0)\n",
    "        cf_score = cf_dict.get(course_id, 0)\n",
    "        \n",
    "        # Weighted combination\n",
    "        final_score = content_weight * content_score + cf_weight * cf_score\n",
    "        \n",
    "        hybrid_scores.append((course_id, final_score, content_score, cf_score))\n",
    "    \n",
    "    # Sort by final score\n",
    "    hybrid_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return hybrid_scores[:top_n]\n",
    "\n",
    "# Test hybrid model\n",
    "sample_recs_hybrid = hybrid_recommend(sample_user, train_df, courses, course_similarity, svd_model, top_n=5)\n",
    "\n",
    "print(f\"üìù Hybrid recommendations for User {sample_user}:\")\n",
    "print(f\"   (Weights: {0.6:.1f} content + {0.4:.1f} CF)\\n\")\n",
    "for course_id, final_score, content_score, cf_score in sample_recs_hybrid:\n",
    "    course_name = courses[courses['course_id'] == course_id]['Course Name'].values[0]\n",
    "    print(f\"   {course_name[:40]}\")\n",
    "    print(f\"      Final: {final_score:.4f} (Content: {content_score:.4f}, CF: {cf_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7ce64",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "\n",
    "Evaluate all three models using Precision@K and Recall@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(recommend_function, test_data, k_values=[5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation model using Precision@K and Recall@K.\n",
    "    \n",
    "    Args:\n",
    "        recommend_function: Function that returns recommendations for a user\n",
    "        test_data: Test interaction dataframe\n",
    "        k_values: List of K values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision and recall for each K\n",
    "    \"\"\"\n",
    "    results = {k: {'precision': [], 'recall': []} for k in k_values}\n",
    "    \n",
    "    # Group test data by user\n",
    "    test_by_user = test_data.groupby('user_id')['course_id'].apply(set).to_dict()\n",
    "    \n",
    "    for user_id in test_by_user:\n",
    "        # Get recommendations\n",
    "        try:\n",
    "            recommendations = recommend_function(user_id)\n",
    "            \n",
    "            if not recommendations:\n",
    "                continue\n",
    "            \n",
    "            # Get actual courses from test set\n",
    "            actual_courses = test_by_user[user_id]\n",
    "            \n",
    "            for k in k_values:\n",
    "                # Get top K recommendations\n",
    "                if len(recommendations[0]) == 4:  # Hybrid format\n",
    "                    top_k = [rec[0] for rec in recommendations[:k]]\n",
    "                else:  # Regular format\n",
    "                    top_k = [rec[0] for rec in recommendations[:k]]\n",
    "                \n",
    "                # Calculate hits\n",
    "                hits = len(set(top_k) & actual_courses)\n",
    "                \n",
    "                # Precision@K = hits / K\n",
    "                precision = hits / k if k > 0 else 0\n",
    "                \n",
    "                # Recall@K = hits / total_relevant\n",
    "                recall = hits / len(actual_courses) if len(actual_courses) > 0 else 0\n",
    "                \n",
    "                results[k]['precision'].append(precision)\n",
    "                results[k]['recall'].append(recall)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Average results\n",
    "    for k in k_values:\n",
    "        results[k]['precision'] = np.mean(results[k]['precision']) if results[k]['precision'] else 0\n",
    "        results[k]['recall'] = np.mean(results[k]['recall']) if results[k]['recall'] else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üìä Evaluating models on test set...\\n\")\n",
    "print(\"   This may take a few minutes...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40c20a",
   "metadata": {},
   "source": [
    "### 6.1 Evaluate Content-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c185631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Evaluating Content-Based Model...\")\n",
    "\n",
    "content_results = evaluate_model(\n",
    "    lambda uid: content_based_recommend(uid, train_df, courses, course_similarity, top_n=10),\n",
    "    test_df\n",
    ")\n",
    "\n",
    "print(\"\\n   Content-Based Results:\")\n",
    "for k, metrics in content_results.items():\n",
    "    print(f\"      @{k}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263024df",
   "metadata": {},
   "source": [
    "### 6.2 Evaluate Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ù Evaluating Collaborative Filtering Model...\")\n",
    "\n",
    "cf_results = evaluate_model(\n",
    "    lambda uid: collaborative_recommend(uid, train_df, courses, svd_model, top_n=10),\n",
    "    test_df\n",
    ")\n",
    "\n",
    "print(\"\\n   Collaborative Filtering Results:\")\n",
    "for k, metrics in cf_results.items():\n",
    "    print(f\"      @{k}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dec8cc",
   "metadata": {},
   "source": [
    "### 6.3 Evaluate Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa060067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Evaluating Hybrid Model...\")\n",
    "\n",
    "hybrid_results = evaluate_model(\n",
    "    lambda uid: hybrid_recommend(uid, train_df, courses, course_similarity, svd_model, \n",
    "                                 content_weight=0.6, cf_weight=0.4, top_n=10),\n",
    "    test_df\n",
    ")\n",
    "\n",
    "print(\"\\n   Hybrid Model Results:\")\n",
    "for k, metrics in hybrid_results.items():\n",
    "    print(f\"      @{k}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf28ad9",
   "metadata": {},
   "source": [
    "## 7. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Content-Based',\n",
    "        'Precision@5': content_results[5]['precision'],\n",
    "        'Recall@5': content_results[5]['recall'],\n",
    "        'Precision@10': content_results[10]['precision'],\n",
    "        'Recall@10': content_results[10]['recall']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Collaborative Filtering',\n",
    "        'Precision@5': cf_results[5]['precision'],\n",
    "        'Recall@5': cf_results[5]['recall'],\n",
    "        'Precision@10': cf_results[10]['precision'],\n",
    "        'Recall@10': cf_results[10]['recall']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Hybrid (0.6 + 0.4)',\n",
    "        'Precision@5': hybrid_results[5]['precision'],\n",
    "        'Recall@5': hybrid_results[5]['recall'],\n",
    "        'Precision@10': hybrid_results[10]['precision'],\n",
    "        'Recall@10': hybrid_results[10]['recall']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_p5 = comparison_df.loc[comparison_df['Precision@5'].idxmax(), 'Model']\n",
    "best_r5 = comparison_df.loc[comparison_df['Recall@5'].idxmax(), 'Model']\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMERS:\")\n",
    "print(f\"   Precision@5: {best_p5}\")\n",
    "print(f\"   Recall@5: {best_r5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f49613",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06783e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Precision comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, comparison_df['Precision@5'], width, label='Precision@5', color='#2E86AB')\n",
    "ax1.bar(x + width/2, comparison_df['Precision@10'], width, label='Precision@10', color='#6A994E')\n",
    "ax1.set_xlabel('Model', fontweight='bold')\n",
    "ax1.set_ylabel('Precision', fontweight='bold')\n",
    "ax1.set_title('Precision Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Recall comparison\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x - width/2, comparison_df['Recall@5'], width, label='Recall@5', color='#F18F01')\n",
    "ax2.bar(x + width/2, comparison_df['Recall@10'], width, label='Recall@10', color='#A23B72')\n",
    "ax2.set_xlabel('Model', fontweight='bold')\n",
    "ax2.set_ylabel('Recall', fontweight='bold')\n",
    "ax2.set_title('Recall Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd81b0",
   "metadata": {},
   "source": [
    "## 9. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b583b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving models and results...\\n\")\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save content-based components\n",
    "with open('../models/content_based_model.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'vectorizer': vectorizer,\n",
    "        'similarity_matrix': course_similarity\n",
    "    }, f)\n",
    "print(\"   ‚úÖ Saved: content_based_model.pkl\")\n",
    "\n",
    "# Save CF model\n",
    "with open('../models/cf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svd_model, f)\n",
    "print(\"   ‚úÖ Saved: cf_model.pkl\")\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'content_based': content_results,\n",
    "    'collaborative_filtering': cf_results,\n",
    "    'hybrid': hybrid_results,\n",
    "    'comparison_table': comparison_df.to_dict('records'),\n",
    "    'hybrid_weights': {'content': 0.6, 'cf': 0.4}\n",
    "}\n",
    "\n",
    "with open('../models/evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "print(\"   ‚úÖ Saved: evaluation_results.json\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('../models/evaluation_results.csv', index=False)\n",
    "print(\"   ‚úÖ Saved: evaluation_results.csv\")\n",
    "\n",
    "print(f\"\\n‚ú® ALL MODELS SAVED!\")\n",
    "print(f\"   Location: ../models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04123fdf",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### What We Built\n",
    "1. ‚úÖ **Content-Based Model**: TF-IDF + cosine similarity on course skills\n",
    "2. ‚úÖ **Collaborative Filtering**: SVD matrix factorization on user interactions\n",
    "3. ‚úÖ **Hybrid Model**: Weighted combination (60% content + 40% CF)\n",
    "\n",
    "### Evaluation\n",
    "- Tested on 20% held-out interactions\n",
    "- Metrics: Precision@K and Recall@K\n",
    "- Compared all three models\n",
    "\n",
    "### Next Steps\n",
    "- Deploy the best-performing model\n",
    "- Build API endpoints for recommendations\n",
    "- Add explainability (why this recommendation?)\n",
    "- Collect user feedback to improve models üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
